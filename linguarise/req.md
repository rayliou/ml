
q2: so list the appropriate models we can use, including its input length, model size, 
how to map the segment audio to letter
and so on
q3: based on the shadow learning product requirements, you need to do a deep research.
- figure out the model candidates that have been pertained so that I don't have to be involved in the training trap
- don't mask the mistake by auto-correct
- support living microphone audio 

q4: You know, our goal is to provide a score for each word in the standard reading. So, our input is a .wav file with a text file. The .wav file would be recorded by the student. And the text file is generated from the original .wav file provided by the material owner. In this case, based on the discussion above, you talk about MFA and kaldi. Based on these two technologies, you need to figure out a step-by-step solution for the speech score 
q5: Did you handle the missing or the error word? For example, undid, new word, or deleted. Unexisting word in the student website. My point is, if the pronunciation turns out to be an error word, or under some new word, deleted some word, or even exchanged the sequence, did you consider about it?
q6: Since we have the both teacher and student audio together, and the reference transcript, let's consider a scenario. We can use MFA Aligner to align the teacher audio with the reference text transcription. And then, align the student's audio and their transcript. Now, we have two aligned files. Then, based on these two aligned files, we can calculate the related 音素序列. And then, we can compare the audio directly using some normalization mechanism. And then, we can provide the score. You can do some analysis and review to provide your idea.
q7: Do you think this is the best practice for the requirements we discussed above? You need to evaluate not only the performance, but also the technology, the engineering, implementation, challenge, all of the aspects.
q8: Actually, our solution includes several parts. I prefer some parts run on devices such as Android, iOS, or iPad. So, the C++ version could be better. And the heavier part, we would use API to provide the service functionality in the prototype phase. Maybe we use Python, conda, Python environment to implement them. Eventually, in the future, we still need update to C++ or even to CUDA GPU accelerates things or something step by step. So, you need review the above solution under the practice. Which technology stack I should use. Remember, our goal is to implement the Shandong Learning's goal. We need to detect the mispronunciation and omits. And then, provide your idea and your insight.
q9: 
=====================================================================

我要建一个英语学习产品，先从影子跟读入手，但未来涵盖英语学习各方面。你需要考虑英文文化历史，起个很有意义的名字

suppose you are a great ASR product manager, let's focus on the below scenario
- create a shadow learning system for language learning.
- automatically continuous inference the streaming audio from microphone and transcribe it to text.
- we should not use the ASR model with very powerful error correcting capability which will conceal the pronouncing error from the learner. instead , we need a model that
- balance the WER and the audio input to correct the speaker's pronunciation
- mapping the audio range to the specific letter to point out the exact error location
- the model support the streaming living predict

We can use the Kaldi Vosk or Montreal Forced Aligner, 

We have discussed a lot of models and tech stacks above for the shadow learning application development. now you need to figure out the best solution , tech stack, model  based on we have discussed and your new search results.

1. the primary deployment env are mobile phone , iPad and PC desktop. actually  cloud based could be another choice
2. as long as the speed is not an issue, cloud base is not a big deal. however, offline solution works as well.
3. yes, for real time feedback

=====================================================================
V1

# Best ASR Models for Real-Time “Shadowing” Pronunciation Feedback

To meet the **shadow learning** requirements, we compare several open-source ASR models and frameworks – both classic and cutting-edge. We evaluate each on architecture, language support, literal transcription (minimal auto-correction), streaming capability, deployment, and suitability for **English, French, Spanish** across devices. Finally, we recommend an optimal solution combining accuracy with real-time feedback clarity.

## Candidate ASR Models Comparison

### OpenAI Whisper (Transformer Encoder-Decoder)

* **Architecture:** Sequence-to-sequence Transformer (encoder-decoder) trained on **680k hours** of multilingual data. Whisper’s large-scale training yields **human-level** robustness to accents and noise. It’s offered in various sizes (Tiny to Large, 39M–1.5B params).
* **Language Support & Decoding:** Natively **multilingual** (over **95 languages** including English, French, Spanish). The model uses an **autoregressive decoder** (no CTC) that predicts text tokens with a built-in language model. This lets Whisper handle multilingual transcription and even translation tasks in one model.
* **Accuracy vs. Literalness:** Whisper delivers **state-of-the-art accuracy** (lowest WER on many benchmarks). It is highly robust to different accents and can even add punctuation/casing. However, its strong internal language model means it may **“auto-correct”** subtle errors – e.g. guessing the intended word from context. This can hide mispronunciations (Whisper might output a *correct* word even if pronounced imperfectly). In practice, Whisper’s transcripts are very clean, but less “literal” to the raw audio because the model favors valid words/grammar.
* **Streaming Support & Latency:** **Not designed for true streaming** – Whisper processes audio in chunks (up to 30 seconds). It cannot natively output partial results word-by-word. Some third-party solutions achieve near-real-time by splitting audio into smaller overlapping windows, but there is inherent **latency**. Whisper (especially large models) is also **computationally heavy** – it’s **extremely accurate but an order of magnitude slower** than lighter models like wav2vec 2.0. Real-time use demands a powerful GPU or using smaller Whisper models. For example, Whisper Tiny/Base can approach real-time on CPU, but larger models will lag without acceleration.
* **Deployment Feasibility:** Despite the heavy model, Whisper is open-source and has a thriving ecosystem. Official implementation is Python (PyTorch). For deployment, **community optimizations** exist: e.g. `whisper.cpp` (C++ with 4-bit quantization) can run Whisper on mobile/embedded devices, and ONNX or CoreML conversions are available. There are **no official ONNX/WASM** releases from OpenAI, but projects like Whisper.cpp and FasterWhisper (CTranslate2) fill this gap. These enable Whisper to run cross-platform (desktop, iOS, Android) albeit with reduced model size and some speed-up. Still, on mobile CPUs, only the smallest models will run smoothly (Tiny \~ <20 MB 4-bit) – larger ones may be too slow for real-time.
* **Best Use Cases:** **Cloud or high-end PC** deployment is where Whisper shines – a GPU-backed server can stream audio to a Whisper Large model and return highly accurate transcripts for English/French/Spanish. This gives learners the best accuracy (catching most errors except very subtle pronunciation issues). On-device, Whisper **Small or Tiny** model can be used if needed (for offline mode), but expect lower accuracy and limited feedback for those smaller models. Whisper’s lack of built-in streaming means that if real-time feedback (word-by-word as you speak) is critical, a different streaming-capable model or a clever chunking strategy is required. In summary, use Whisper for **accuracy** (especially in cloud/PC scenarios), but be mindful of its **latency and mild autocorrect tendencies**.

### Vosk (Kaldi-Based ASR)

* **Architecture:** Vosk is built on **Kaldi**’s traditional ASR pipeline. It uses a pretrained acoustic model (typically a TDNN or CNN-TDNN neural network) with an HMM/DNN hybrid approach and a fixed **WFST decoder** (finite-state transducer) integrating a pronunciation lexicon and n-gram **language model**. This classic architecture breaks speech recognition into acoustic modeling + decoding, unlike end-to-end transformers.
* **Language Support & Decoding:** Vosk supports **20+ languages and dialects** out-of-the-box – including **English, French, Spanish, Portuguese, Russian, Chinese, Turkish, etc.** You download a separate model for each language (model sizes vary: “small” \~50–100 MB and “large” 300+ MB). Decoding is based on words in a vocabulary; the n-gram LM helps choose valid word sequences. Vosk will only output words from its vocabulary (though you *can* extend it with custom words). This means it **won’t invent non-word spellings** – an unknown or badly mispronounced word may be skipped or confused with the nearest sounding known word.
* **Accuracy vs. Literalness:** Vosk’s accuracy is good on clear speech but generally **below neural end-to-end models** like Whisper. (In one comparison, Vosk was less accurate than Facebook and NVIDIA models on English benchmarks.) However, its more constrained decoding can be an advantage for **literalness**: it doesn’t apply aggressive autocorrection beyond its n-gram model. If a learner’s pronunciation is off, Vosk might output a different word (or no word) rather than “figure out” the intended word. For example, a strong mispronunciation that sounds like a different word will likely be transcribed as that wrong word – revealing the error. On the other hand, minor accent issues that still fall within the acoustic model’s range will output the correct word (so some subtle mispronunciations might not be flagged). Vosk’s lexicon-based decoding ensures real words, so you won’t get random gibberish characters – but this also means it can’t explicitly show a mispronounced word’s phonetic spelling. In short, Vosk provides **moderate accuracy** and will expose many mispronunciations by outputting *incorrect words* or gaps when it fails to recognize speech.
* **Streaming & Latency:** **Streaming is a core feature** of Vosk. It’s optimized for **real-time** on CPU, even on lightweight devices. Vosk provides APIs for streaming microphone input with *partial results*: as you speak, you get interim transcripts, and then a final result after a pause. Latency is low – the small models can run faster than real-time on modern mobile CPUs, and even the large models achieve real-time on desktops. Since it’s an on-device offline engine, there’s no network delay. Vosk’s streaming decoder can output a few words with only a short delay (a few hundred milliseconds).
* **Deployment:** Vosk is very **deployment-friendly**. It’s written in C++ with bindings for **Python, Java, JavaScript, C#, etc.**, and even community Flutter plugins. It supports **iOS, Android, Linux, Windows** – truly cross-platform. Integrating Vosk is straightforward: you include the library and a model file. Memory footprint is reasonable (small models <100 MB). No specialized hardware needed – it runs on CPU. This makes Vosk ideal for mobile or embedded offline use. It does not require training or complex setup – models are prepackaged and ready to go.
* **Best Use Cases:** **Mobile and offline scenarios** benefit greatly from Vosk. For a shadowing app, you could embed Vosk to provide real-time feedback **on iPads or Android phones** without internet. For each target language, load the corresponding model. On desktops, Vosk is also easy to use for live dictation, though one might prefer more accurate models if available on PC. Vosk’s trade-off: it’s not the most accurate model, especially for conversational or slurred speech, but it gives **immediate, on-device results** and has minimal autocorrect. This means learners get fast feedback and many pronunciation mistakes will show up as transcription errors (which is what we want to flag). Overall, Vosk is a strong choice for **streaming, low-latency offline ASR** in English/French/Spanish, with the caveat that its recognition accuracy is mid-tier by today’s standards (which could miss some words or mis-transcribe, especially for heavy accents or fast speech).

### K2 Sherpa-ONNX (Next-gen Kaldi with Transducer Models)

* **Architecture:** **Sherpa-ONNX** is part of the K2 project (Kaldi’s successor) and focuses on efficient **end-to-end models**. It primarily uses **RNN-Transducer (RNN-T)** and related architectures (e.g. **Zipformer**, a zip-convolution transformer encoder) exported to ONNX for runtime. Some Sherpa models use **Paraformer** (non-autoregressive transformer with CTC-like modeling). In essence, Sherpa leverages modern **E2E architectures** (no HMM) designed for streaming, but packaged in a lightweight C++/ONNX runtime. The model sizes range from \~20M parameters (small footprint) to larger models, often with quantized INT8 options for speed.
* **Language Support & Decoding:** Sherpa-ONNX is relatively new, and most **pretrained models** available focus on English and Mandarin (and combinations) – e.g. bilingual Chinese+English Zipformer, a multilingual Asian languages model. It does **not yet have off-the-shelf French/Spanish** models as of 2024 (those would require training or exporting a model trained on those languages). However, the framework can in principle run any ONNX ASR model. Decoding in Sherpa depends on the model type: for **Transducer models**, it performs joint acoustic + language prediction (the RNN-T “predictor” acts like a built-in LM, but a small one); for **CTC models** (like Paraformer or CTC segments), it can decode via beam search over character/word pieces. In both cases, the decoding is **open vocabulary** (not limited to a fixed dictionary) – the model itself learned to output text. This means Sherpa-based ASR can output arbitrary word piece sequences (it might output an OOV word spelled out if the model learned to spell). The internal LM in transducer is weaker than a large external LM, so it likely won’t over-correct heavily, but still ensures outputs look like plausible words.
* **Accuracy vs. Literalness:** The accuracy of Sherpa’s models is **high** for the languages they support – e.g. its English models (trained on LibriSpeech or GigaSpeech) reach low WERs (\~5% on Libri test-other in some cases) comparable to other top models. For our purposes, a Sherpa RNN-T model would strike a balance: better raw accuracy than Kaldi/Vosk (due to end-to-end training), but also fairly **literal** since there’s no big external language model. RNN-T will output what it “hears” with some influence of predictor (which can help correct obvious incomplete words, but not to the extent of Whisper’s massive LM). So small mispronunciations might result in either a wrong word or a slightly muddled word-piece sequence. (If using a CTC model with no LM, it could even produce a sequence of letters approximating the mispronunciation.) This means Sherpa-based solutions can potentially **expose mispronunciations** while still being accurate on well-spoken input. In practice, their reliability for French/Spanish would depend on training – since pretrained models for those aren’t provided, one might have to use an English model (not useful for French/Spanish speech) or obtain a multilingual model elsewhere.
* **Streaming & Latency:** **Streaming is a key design goal** of Sherpa-ONNX. RNN-T models inherently output tokens as audio frames arrive, and Sherpa’s framework supports low-latency, partial result emission. It even provides a WebSocket server/client for streaming ASR. Latency is very low – the Zipformer-based models are optimized for fast inference on CPU, often achieving real-time on modest hardware. For example, small Sherpa models can run in <0.5x realtime on modern ARM devices (meaning they process audio faster than it’s spoken). The use of ONNX Runtime allows hardware acceleration (e.g. neural network inference can leverage SIMD or GPU delegates). In short, Sherpa can deliver **real-time transcription** comparable to Vosk’s latency, but using advanced end-to-end models.
* **Deployment:** Sherpa-ONNX is highly **cross-platform** and developer-friendly. It is self-contained in C++ and uses ONNX Runtime as the backend. Importantly, it offers **bindings in many languages** (C, C++ API, Python, Java, JavaScript/TypeScript, Kotlin, Swift, Go, C#/.NET, etc.) – even WASM for browser and mobile-specific builds for Android/iOS. This wide support is a huge plus for integrating into different app environments. You can run Sherpa-ONNX on **PC, mobile, and even embedded Linux**. Models can be quantized to INT8 to reduce footprint. Since it doesn’t require an internet connection (everything runs locally on-device) and is optimized for streaming, it’s very suitable for real-time feedback apps. The main deployment *challenge* is obtaining a suitable model for French and Spanish – one would likely need to convert or fine-tune a model and export to ONNX. If that hurdle is overcome, the tech stack itself is modern and actively maintained.
* **Best Use Cases:** **English streaming ASR on all platforms** is where Sherpa-ONNX currently excels – for instance, a bilingual English-Chinese learning app could use the provided models to give offline feedback. If French/Spanish models become available (or if one leverages a community model exported to Sherpa), this could be a **one-stop solution**: a unified API across desktop, web, and mobile with efficient streaming and good accuracy. It’s especially attractive for a cross-platform app due to its **consistent SDK and active development**. In comparison to Vosk, Sherpa’s models (when available) tend to be more accurate and faster (thanks to newer architectures), and the project is under active development by the speech community. **However**, until pretrained models for FR/ES are accessible, Sherpa-ONNX might require additional effort (model training or conversion), which might not be feasible if “no training” is a hard requirement. Keep an eye on Sherpa for future multilingual model releases – it could potentially become the best literal, streaming ASR for many languages.

### Facebook Wav2Vec 2.0 (and Hugging Face Models)

* **Architecture:** **Wav2Vec 2.0** is a self-supervised **Transformer encoder** model (by Facebook/Meta) that learns speech representations from unlabeled audio. After pretraining on audio alone, it’s fine-tuned with a CTC loss on transcribed speech for ASR. The typical architecture: a CNN feature extractor + Transformer blocks (95M–300M params depending on base/large) feeding into a linear CTC decoder (character or word-piece). There’s no separate language model in the end-to-end model, though one can be applied externally.
* **Language Support & Decoding:** Meta released **multilingual Wav2Vec2** models (e.g. XLSR-53 which covers 50+ languages including English, French, Spanish). By fine-tuning these on specific language data, the community has produced high-quality models for many languages. For example, there are pre-trained weights for Spanish (trained on CommonVoice ES) and French, available on Hugging Face. Decoding is usually via **CTC (Connectionist Temporal Classification)**, meaning the model directly emits character or subword sequences aligned to the audio. Without an external language model, Wav2Vec2’s decoding is relatively **literal** – it tries to spell out what it hears. You can also decode with a lexicon and n-gram LM (some HF models provide a KenLM) to improve accuracy, but that can introduce autocorrection. Many open-source Wav2Vec2 models choose to decode greedily or with a small beam, so they tend to output what was spoken, including mistakes (albeit often missing punctuation/casing).
* **Accuracy vs. Literalness:** When fine-tuned on large datasets, Wav2Vec2 achieves very **strong accuracy** – often on par with other top models. (E.g., Wav2Vec2-large fine-tuned on 960h LibriSpeech had \~2% WER on test-clean with LM, \~5.8% without, and multilingual models reach <10% WER on many languages). In practice, Whisper has surpassed Wav2Vec2 in multilingual robustness, but Wav2Vec is still very competitive and sometimes *more verbatim*. Because it uses CTC, it has no built-in way to insert words that weren’t sounded out – it largely outputs the sequence of sounds as letters. This means mispronounced words may result in **misspelled or garbled text**, clearly indicating an error. For example, if a learner says *“español”* incorrectly, a Spanish Wav2Vec2 might output a similarly sounding nonsense word or a badly spelled version, whereas a model with a strong LM might still output “español” correctly. Thus, Wav2Vec2 can be **literal to a fault**: great for catching errors, though sometimes it may produce an odd transcript if it’s unsure. Its accuracy will drop with heavy accents or noise (where Whisper’s huge training helps), but it doesn’t intentionally correct grammar or vocabulary beyond what training data taught it.
* **Streaming & Latency:** Vanilla Wav2Vec 2.0 is **not inherently streaming-friendly** – it’s normally used on complete utterances. The Transformer encoder looks at the whole input chunk (or a large context) which introduces latency. That said, it’s possible to implement *chunked streaming*: processing audio in blocks (e.g. 1-2 seconds) with overlap. This is similar to how one might stream Whisper, and some research has been done on streaming Wav2Vec2 by restricting its attention context. There isn’t an official streaming Wav2Vec2 API, so implementing real-time feedback with partial results is non-trivial. You might only get the final transcription after each phrase or chunk, rather than word-by-word live subtitles. In terms of speed, Wav2Vec2 (base model) is **faster than Whisper** because it’s a simpler encoder-only model. On CPU, a base 95M param model can run in real-time or better for short utterances, especially if optimized (ONNX Runtime or OpenVINO etc.). Larger models or multilingual ones (\~300M) are slower but can possibly still approach real-time on a desktop CPU. On mobile, Wav2Vec2 is heavy – you’d need to quantize and maybe use only smaller variants (or distilled models) for acceptable latency.
* **Deployment:** Many Wav2Vec2 models are available via **Hugging Face Transformers**, which makes experimentation easy in Python. For production, one can export the model to **ONNX** or TorchScript. ONNX Runtime can then run it on various platforms, and even accelerate with NNAPI (Android) or CoreML (iOS via converting to CoreML format). There have been community projects running Wav2Vec2 on mobile (e.g., Apple demonstrated a CoreML conversion for on-device ASR). However, unlike Vosk or Sherpa, there isn’t a ready-made C++ library with streaming support – you’d be rolling your own inference pipeline. For the web, running a large Wav2Vec in WebAssembly is tough (Whisper models are actually more optimized for that via whisper.cpp). In summary, deployment is **possible but custom**: you benefit from the open model availability, but you need to handle integration, optimize for speed (quantization, hardware delegate), and possibly manage chunking for streaming.
* **Best Use Cases:** **High-accuracy transcription in a controlled setting** – for instance, processing recordings or giving feedback after a student finishes a sentence. If real-time word-by-word feedback is not absolutely required, Wav2Vec2 models for English/French/Spanish could be used locally on a device to transcribe each phrase the user speaks, and then compare to the expected text. They will reliably show mispronounced words as errors in the transcript, given no strong LM is masking them. On mobile offline, Wav2Vec2 is harder to use than Vosk; on a PC (or cloud), it’s easier (with GPU or even CPU). If one wanted a single model to cover all three languages, a multilingual Wav2Vec2 (XLSR) could be loaded, but note that a combined model might be large and slightly less accurate per language than monolingual ones. Overall, Wav2Vec2 (and its many fine-tuned derivatives) is a **powerful ASR option** with open-license models, but for a shadowing app you would need to solve the streaming output challenge or accept giving feedback per utterance rather than truly continuous transcript.

### ESPnet (End-to-End Toolkit – e.g. Conformer models)

* **Architecture:** **ESPnet** provides many E2E model architectures, notably **Transformer and Conformer** encoder-decoders for ASR. A common recipe is a Conformer encoder with hybrid CTC/Attention decoding. ESPnet models often use an attention decoder (like LAS) *plus* a CTC branch for stability, or even RNN-Transducer in ESPnet2. Essentially, it’s a SOTA research toolkit – for example, ESPnet’s Conformer models achieved <5% WER on Librispeech and support voice activity detection, etc. It also integrates pretrained models from other sources.
* **Language Support & Decoding:** Through ESPnet’s model zoo, one can find models for **multiple languages** – English is well-covered, and there are models for Japanese, Chinese, and some for Spanish, French (often trained on Common Voice or similar). However, these may not be as well-documented as Whisper or Vosk models. Decoding style depends on the model: many use **joint CTC/attention** decoding, which means during inference you might combine a CTC output with an attention decoder output. This typically requires a beam search and possibly a small language model to resolve choices. There are also pure CTC models in ESPnet and RNN-T models. In general, an ESPnet model can be configured to output very literally (CTC greedy output) or with an integrated LM for more accuracy. By default, their published models aim for high WER performance, so they might include some amount of LM bias (making them output proper words).
* **Accuracy vs. Literalness:** Models trained via ESPnet are usually **highly accurate** on their training domain – often reaching benchmark-leading scores. For example, a Conformer model trained on Librispeech (English) or on Common Voice (for other languages) will have strong recognition capabilities. In terms of literalness, if you use the attention decoder output, it is similar to Whisper in that it’s sequence-to-sequence and could correct minor misspeakings. If you rely on the CTC branch alone, it would output characters more literally but possibly with more errors. The level of autocorrection can be adjusted (some ESPnet inference allows weighting CTC vs attention vs LM). This flexibility is powerful but requires tuning. Mispronunciations would likely be caught if you favor the acoustic evidence (CTC) – the model might output an incorrect word or a partial word. But given these models are trained on native speech mostly, heavy accent or mispronounced input might confuse them and cause deletions or random output.
* **Streaming & Latency:** **Streaming support** in ESPnet is available in ESPnet2, but it isn’t the default for all models. There are specific streaming Conformer variants (using chunked attention) and streaming Transformer demos. If a model is trained/configured for streaming, it can process audio in real-time chunks, outputting partial hypotheses. However, many pretrained models are *offline* (they look at the whole sentence). Using an offline model in streaming mode can introduce large delays or degrade accuracy. So, unless you find a “streaming ASR” model in ESPnet’s zoo explicitly, you’d need to modify a model or accept higher latency. In terms of speed, ESPnet models can be large (100M+). On CPU, some may not run real-time unless optimized; on GPU they’re fine. Latency also comes from the beam search decoding step.
* **Deployment:** ESPnet is primarily a **Python toolkit** for research. It doesn’t have a production-focused runtime like Vosk or Sherpa built-in. However, you can export models to ONNX or use TorchScript. Some ESPnet models are available on Hugging Face, which means you could leverage the Transformers or 🐸 **SpeechBrain** interface to run them. Still, deploying an ESPnet model to mobile would require conversion to TFLite or ONNX and writing your own inference code. There isn’t an official mobile library. Community contributions aside, ESPnet’s strength is flexibility and state-of-the-art performance, not ease of deployment. Maintenance is active (it’s an ongoing project), but mostly by academic contributors. Cross-platform use would be via the exported model (so similar effort as deploying a Wav2Vec2 or other PyTorch model).
* **Best Use Cases:** If you need a **specific pre-trained model** that only ESPnet provides (say a high-quality French model or a bilingual model for code-switching) and are willing to do some integration work, ESPnet is a great resource. For a general shadowing app, though, ESPnet might be overkill – you’d essentially pick an ESPnet-trained model and then deploy it similar to any custom model. It might make sense if, for example, their **Conformer model for Spanish** gives better accuracy on conversational Spanish than available alternatives. But one must consider the development cost: you won’t have the plug-and-play simplicity of other stacks. In summary, ESPnet offers **top-notch ASR models** for many languages (including our target ones) and can be made to do streaming, but using it in a product likely means more custom work. It may be preferable to take an ESPnet model and convert it into a more deployment-friendly format (like ONNX) and run it via a framework such as Sherpa-ONNX or your own inference loop.

### Coqui STT (Mozilla DeepSpeech 2 Legacy)

* **Architecture:** **Coqui STT** (formerly Mozilla DeepSpeech) uses a deep **bidirectional LSTM RNN** with a CTC output layer, inspired by Baidu’s DeepSpeech2. The model is relatively small (the English model is \~50 MB). It outputs letters or characters which are then combined into words. An optional external KenLM language model can be used to improve word accuracy.
* **Language Support & Decoding:** Coqui provided a high-quality **English** model pretrained on American English speech. They also enabled transfer learning to other languages, and some community models exist (e.g. there have been models for German, Spanish, etc., trained on Common Voice). However, Coqui’s official model zoo has been taken offline as they shifted focus. If you have access to released models (they’re still on GitHub releases), you could get models for our target languages, but they may not be as refined. Decoding is pure **CTC**: the acoustic model emits character probabilities frame by frame, which are then decoded to text. With **no external LM**, the raw output will be the model’s best guess spelled out, which can include misspelled words if the pronunciation is unclear. With the LM enabled (a separate binary file), the decoder will bias toward valid words. You can control this with a tunable LM weight.
* **Accuracy vs. Literalness:** The accuracy of DeepSpeech/Coqui STT on clear speech is moderate – for example, \~7-10% WER on LibriSpeech test-clean with the LM. It’s worse on more challenging audio (the model was smaller and trained on fewer hours than Whisper or wav2vec). This means it will make mistakes even on correctly pronounced words sometimes. On the flip side, it **does not strongly autocorrect**. If you **disable the LM**, the system will output very literal character-by-character recognition – which often results in phonetically spelled errors for mispronounced words. For instance, if someone said *“cat”* but pronounced the *“a”* oddly, the raw output might be “cot” or “ct”. This raw output can highlight pronunciation issues (but might be hard to read). Typically, one would keep a mild LM to ensure outputs are mostly real words but not too aggressively corrected. Coqui STT will definitely show major mispronunciations as either wrong words or gibberish, making it useful to catch errors, though its overall lower accuracy means it might also misunderstand correct speech (false positives).
* **Streaming & Latency:** **Streaming inference is supported** – the RNN model processes audio sequentially, and Coqui provides an API to feed audio chunks and receive partial transcripts in real-time. It was designed for real-time dictation, so latency is low. The model runs fast on CPU (written in TensorFlow/C++ and there’s a TFLite version). Even on a Raspberry Pi, DeepSpeech could run in real-time, so mobile devices can handle it. The partial results may be unstable (characters can change as more context arrives, typical for an online CTC), but you do get immediate feedback as you speak.
* **Deployment:** Coqui STT was **multi-platform** – it had C++ binaries and wrappers, and even a ready **TFLite model** for mobile. You could run it on Android or iOS using the TensorFlow Lite runtime. There was also a Node.js wrapper. So deployment *was* relatively straightforward. The big caveat: **Coqui STT is no longer actively maintained** by the company. The latest version is still usable, but you won’t get future updates or support. Given the project’s maturity, it’s stable to use as-is, but any bugs or compatibility issues with new OSes might not be fixed. Community support exists (since it’s open source under MPL).
* **Best Use Cases:** Coqui STT could be a solution if you need an **offline, lightweight engine** and Whisper/Vosk don’t meet some criteria. It’s essentially an alternative to Vosk for on-device use. For English it works out-of-the-box; for Spanish or French, you’d have to find pretrained models (if available) or fine-tune the English model with some data (requires training, which you wanted to avoid). Considering it’s deprecated in favor of newer models, one would typically prefer Vosk or Sherpa which have more momentum. However, if one already has Coqui models for the target languages, it will provide real-time feedback and decent literal transcription. The trade-offs: slightly lower accuracy and an aging codebase. It might expose pronunciation mistakes by outputting weird text, but it might also frustrate with its overall recognition quality compared to newer systems. In summary, Coqui STT is **capable of the task (streaming, cross-platform, open-source)**, but given it’s not maintained and others outperform it, it would usually not be the top choice in 2025.

### Other Notable Options

* **WeNet (AIShell/OpenMS):** An open-source ASR aimed at **production-first** deployment. It offers unified streaming/non-streaming Conformer models and has pretrained models for Chinese and English. WeNet is efficient and mobile-friendly, similar in spirit to Sherpa-ONNX (in fact Sherpa uses some WeNet models like Paraformer). If WeNet had French/Spanish models, it would be worth considering (it currently focuses on Asian languages and English).
* **NVIDIA NeMo and Riva:** NVIDIA’s toolkit provides **Conformer-CTC and Conformer-Transducer** models for many languages (including Spanish, French via Mozilla Common Voice fine-tunes). These models are high quality. Deployment is usually via **Riva** (NVIDIA’s inference server) which is free to use but not open-source in code. If your target devices are NVIDIA GPUs (or Jetson platforms), NeMo models accelerated with TensorRT could give top performance. Without NVIDIA hardware, it’s less appealing.
* **SpeechBrain:** Another all-python toolkit that has some pretrained models (including a multilingual SEQ2SEQ and even a pronunciation scorer). It’s flexible like ESPnet but not specialized for deployment.
* **Picovoice Cheetah/Leopard (Closed-source):** These are offline ASR engines for mobile with low latency. Mentioned for completeness: they are not open-source (commercial licensing), so they don’t meet the requirement, but they exemplify what an optimized mobile ASR can do (with fairly accurate, literal transcription on-device).

In general, the above alternatives either lack ready multilingual models or are more difficult to integrate compared to the main options (Whisper, Vosk, Sherpa, etc.) we detailed.

## Recommended ASR Solution

Taking everything into account – **speed**, **accuracy**, **development effort**, and **feedback clarity** – the optimal approach is to **combine a lightweight streaming engine on the device with a more powerful model in the cloud or as an offline option on powerful hardware**. This hybrid strategy leverages the strengths of each model where appropriate:

* **Mobile (Offline Real-Time):** Use **Vosk** for on-device streaming feedback in English, French, and Spanish. Vosk’s small models can run locally on smartphones/tablets and provide instant partial transcripts. This satisfies the real-time requirement and works without internet. Vosk will expose many pronunciation errors by outputting incorrect words or blanks, and it won’t “magically correct” the user’s speech. Its cross-platform SDK makes integration into iOS/Android straightforward. *Trade-off:* Vosk’s accuracy is decent but not cutting-edge – it might misrecognize some words even if said correctly, especially with background noise or non-American accents. However, for a first-pass feedback to the learner, it performs well enough and ensures truly bad pronunciation is not glossed over. The development effort is low (just embed the library and models). Optionally, you can run slightly larger Vosk models on desktops for better accuracy there, if staying offline on PC.
* **Cloud or Desktop (High Accuracy Analysis):** Use **OpenAI Whisper (Large)** on the backend (or on a PC with GPU) to get a second opinion with **higher accuracy**. For example, when a learner finishes a sentence or when more thorough feedback is needed, audio can be sent to a Whisper model (or run locally on a desktop app with Whisper.cpp or PyTorch). Whisper will produce an extremely accurate transcript for the spoken sentence, which is useful to evaluate overall correctness. Because Whisper is robust, it will understand the learner even with accent – this ensures that if Vosk failed or produced gibberish due to accent, Whisper can catch what was *meant*. However, since Whisper might autocorrect slight mispronunciations, it’s important to **compare its output with Vosk’s output or with the expected text**. Discrepancies can highlight issues: e.g. if Whisper outputs a correct word but Vosk struggled, that may indicate pronunciation was not clear. Whisper supports all three target languages out-of-the-box, so a single model instance can service multilingual input (detecting the language and transcribing). *Trade-off:* Whisper in the cloud introduces some latency (maybe a second or two for short utterances) and requires internet connectivity (or a hefty local install). Also, running Whisper Large can be costly resource-wise. But you gain top-tier transcription quality and a fallback that understands even heavily accented speech (ensuring the app can always get *some* transcript to work with).
* **Feedback Fusion:** By using the **hybrid model approach**, the app can provide two layers of feedback: (1) **Immediate on-device feedback** as the user is speaking (streaming transcript from Vosk), which is motivating and useful for instant self-correction. (2) **Post-utterance detailed feedback** using the more accurate Whisper transcript compared against the target phrase. For instance, the app can highlight words that were different from the expected script – if Whisper and the expected text differ, the learner clearly said something wrong. And if Vosk’s real-time transcript differed in a certain word, that’s also a clue for the learner. This combination covers both real-time interactivity and final accuracy. Development-wise, integrating both isn’t too complex: you’d use Vosk’s SDK on the client side, and call a Whisper-based API for final results. Both models are open-source and widely supported, aligning with the project’s constraints.
* **Alternative Single-Model Solution:** If a hybrid setup is undesired, the next best single solution would be **Whisper on all platforms** using smaller models. For example, Whisper Medium or Small can be converted to **CoreML** for iOS or run via `whisper.cpp` on Android (quantized). This would give a unified model with strong accuracy. You’d get multilingual support and wouldn’t need separate systems. However, you still face Whisper’s lack of true streaming – you might only get results after each phrase rather than word-by-word. And on lower-end devices, even Whisper Small might struggle with real-time processing. So while Whisper-only is attractive for simplicity and accuracy, it may not meet the **immediate feedback** requirement as well, and implementing chunked streaming for Whisper is an extra task. It also consumes more battery/CPU on mobile than Vosk would.
* **Consider Sherpa-ONNX in Future:** As a forward-looking note, keep an eye on **Sherpa-ONNX** developments. If and when reliable French and Spanish ONNX models become available (or if you convert a Wav2Vec2 or ESPnet model to ONNX), Sherpa could replace Vosk as the on-device engine. That would likely boost on-device accuracy and maintain the low latency, with a unified API across platforms. In an ideal scenario, one could even deploy Sherpa-ONNX models for all devices *and* the cloud, simplifying the stack. Currently though, a Vosk+Whisper pairing is more practical given readily available models.

**Why this hybrid?** It offers the best **trade-off** between **speed, accuracy, and development cost**. Vosk gives you a quick, maintained solution for streaming and works offline in all environments (it’s proven tech for mobile speech recognition). Whisper gives you an accurate, multilingual backbone so you don’t miss anything and can handle tricky speech inputs. Both are open-source with active communities, ensuring longevity and support. The division of labor means the app is responsive (thanks to Vosk) and still ultimately correct in its assessments (thanks to Whisper). Learners will benefit from seeing immediate transcriptions (which will tend to reflect their pronunciation literally) and later confirmation and correction from the more advanced model. This dual approach ensures that **pronunciation mistakes are exposed** (not hidden by overzealous autocorrect) while still providing a safety net of accurate recognition. It balances the **clarity of feedback** with the **robustness of understanding**, which is crucial in a language-learning scenario.


















=====================================================================
V0


* Support English, French, and Spanish
* Avoid masking pronunciation errors (i.e., minimal or no automatic correction)
* Can run on PC and mobile with real-time microphone audio input
* Require no additional training or fine-tuning to use

I’ll identify candidate models, their technical specs (model size, language support, decoding method), and how well they expose pronunciation mistakes. I’ll also check for streaming support and ease of integration.

I’ll let you know as soon as I have a list of suitable models and implementation considerations.


# Pretrained ASR Models for English, French, and Spanish

Below we survey several open-source, pretrained automatic speech recognition (ASR) models that meet the criteria: support English, French, and Spanish; minimal language-model bias (to avoid auto-correction of learner errors); real-time streaming capability; and ready-to-use deployment on local PC or mobile devices.

## OpenAI Whisper (Transformer Encoder–Decoder)

* **Model & Size**: Whisper is a sequence-to-sequence Transformer model trained on 680k hours of multilingual data. OpenAI released multiple model sizes ranging from *Tiny* (39 million parameters) up to *Large* (1.55 billion parameters). The largest (Large-v2) is \~3.1 GB in FP16 (\~1.6 GB int8 quantized).
* **Language Support**: It supports 99+ languages, including English, French, and Spanish, in a single model. Whisper can transcribe speech in these languages and even translate from non-English to English using the same model.
* **Decoding Method**: Whisper uses an encoder–decoder Transformer architecture. It directly generates text (with token-by-token beam or greedy decoding) using a decoder that acts as an inherent language model. There is **no separate external LM** – the model was trained end-to-end on transcriptions, so it learns language patterns internally. It tends to produce fluent text and can insert punctuation automatically. This end-to-end approach means it may occasionally normalize or correct inputs based on context, but it does not apply a post-processing spell-check.
* **Streaming Capability**: The model processes audio in chunks (up to \~30 seconds each). Out-of-the-box it is not *truly* streaming (it works on fixed segments), but it can be used for near-real-time transcription by feeding continuous audio in overlapping windows. There are community solutions for live decoding – for example, **whisper.cpp** provides a streaming API to transcribe from a microphone in real-time by processing audio in small blocks. Latency depends on model size – smaller Whisper models can achieve real-time on CPUs, whereas larger models may need GPU acceleration or quantization for live use.
* **Accuracy vs. Literalness**: Whisper is state-of-the-art in accuracy on many benchmarks (e.g. \~5–7% WER on LibriSpeech). It’s robust to accents and noise. Because it was trained on correct transcripts, it tends to output correct words; it might *implicitly* “correct” certain mistakes if the spoken input is close to a real word. However, Whisper does **not** use an external spell-check or grammar model – it transcribes what it hears to the best of its ability. Filler words (“um”, false starts) are usually transcribed, but it may omit or correct blatantly mispronounced words (e.g. a heavily mispronounced word might get transcribed as a more plausible word it sounds like). In general, Whisper’s high accuracy can be a double-edged sword for language learners: it makes few mistakes, but it might not preserve an obvious error if the audio still resembles correct speech.
* **Deployment**: OpenAI open-sourced the code and model weights, so Whisper can run fully offline. It’s integrated in Hugging Face Transformers and PyTorch, and optimized ports like **whisper.cpp** (C++ with INT4/INT8 quantization) enable running on CPUs, Raspberry Pi, and mobile devices. Whisper models can also be converted to ONNX or CoreML for deployment. On a modern PC GPU, the *Large* model transcribes \~0.5× real-time (half the audio speed), while smaller models or quantized versions can run in real-time or faster. On mobile, the tiny or base models are feasible for live transcription with appropriate optimization.

## NVIDIA NeMo Multilingual (FastConformer Transducer)

* **Model & Size**: NVIDIA provides a **FastConformer**-based *Recurrent Neural Network Transducer (RNN-T)* model for ASR. One such model (“FastConformer Hybrid Large”) has \~114 million parameters. It uses a Conformer encoder (Transformer + convolution) optimized for speed, and a joint CTC/RNN-T decoder. There is also a larger 1 billion–parameter model (Canary-1B) with a similar architecture (FastConformer encoder + Transformer decoder) for highest accuracy. The 114M model occupies about 416 MB on disk, whereas the 1B model is several GB.
* **Language Support**: These NeMo models are **multilingual**. For example, the 114M FastConformer model was trained on \~20,000 hours of speech across 10 languages, including English, French, and Spanish. A single checkpoint can transcribe any of the supported languages (it also predicts language ID internally). NVIDIA’s 1B Canary model similarly supports English, French, Spanish, German in one model. This means you don’t need separate models for each language.
* **Decoding Method**: The model uses **joint CTC/Transducer** decoding. It outputs subword tokens (SentencePiece units) with a modeling of blank/next-character as in RNN-T, and was trained with CTC loss as well. There is **no external language model** needed or used during inference – the decoder’s next-token predictions are based only on the acoustic encoder and its own learned probabilities. Notably, the model was trained to emit **punctuation and capitalization** in the transcripts, which is unusual for end-to-end models. Because decoding is subword-based, the system can produce words not seen during training (to some extent) or misspell words that are mispronounced. The integrated decoding makes it end-to-end – you give audio in, get text out.
* **Streaming Capability**: RNN-T models are inherently designed for streaming. NVIDIA’s ASR models support real-time transcription – the *FastConformer* encoder introduces only a small latency. In practice, these models can process audio in streaming fashion, emitting partial results as the user speaks. NVIDIA’s toolkit (NeMo and the Riva SDK) provides streaming APIs, and the RNN-T decoder can produce words character-by-character without needing the full utterance. Thus, this model is well-suited for live microphone input. (In enterprise deployments via **Riva**, Canary-1B is offered with streaming server support.)
* **Accuracy vs. Literalness**: The multilingual RNN-T is quite accurate on its training languages (it topped some open ASR benchmarks as of 2023). Its accuracy is slightly below Whisper on English, but it handles code-switching and multilingual input robustly. Because it doesn’t use an external LM, it will directly output what the acoustic model predicts. In testing, it *can* preserve learner errors to a degree – for instance, if a Spanish speaker mispronounces a word, the output might be a slightly misspelled token or a similar-sounding word (since the decoder chooses the closest matching subword sequence). However, since the training transcripts were mostly correct speech, the model is biased toward valid words. It may not explicitly tag mispronunciations or produce completely nonsensical strings – it will choose some valid token sequence, which could sometimes “correct” the error (e.g. interpreting a mispronounced English word as a different English word that fits acoustically). Overall, it has high recognition performance without a strong artificial correction layer.
* **Deployment**: NVIDIA NeMo models are open-source (checkpoints on NVIDIA NGC and Hugging Face). You can use them directly with the NeMo Python API or export to ONNX. For example, the 114M multilingual model is \~416 MB and can run on a PC GPU or even CPU (with reduced speed). It has been run on Jetson-class devices as well. For mobile, an RNNT of this size is on the heavier side, but it could be converted to TensorRT or a TensorFlow Lite (via ONNX) for optimized inference. NVIDIA’s Riva SDK provides an optimized server (in C++) for these models, which could be used on-premise. In summary, the NeMo multilingual model is ready-to-use in Python and can be integrated into real-time pipelines; deploying on mobile would require some expertise in converting/quantizing the model to fit the device’s constraints.

## Meta Wav2Vec2 (XLS-R) Models via SpeechBrain/HuggingFace

* **Model & Size**: **Wav2Vec 2.0** is a Transformer encoder model (convolutional feature extractor + Transformer layers) pioneered by Meta AI. A typical large wav2vec2 model has \~300 million parameters. Meta released a multilingual pre-trained model (XLSR-53) covering 50+ languages, which can be fine-tuned for specific languages. Using that, open-source projects have produced wav2vec2-based ASR models for English, French, Spanish, etc. For example, SpeechBrain’s French model uses a pretrained wav2vec2 **large** and adds 2 dense layers for CTC decoding, resulting in a model of around 317M params. Disk size can range from \~360 MB (for a large model) to smaller (\~100 MB) for base models.
* **Language Support**: Wav2Vec2 models are generally **monolingual after fine-tuning** (unless explicitly trained on multiple languages). However, the *family* supports English, French, Spanish by using different fine-tuned weights. There are pretrained English models (e.g. on Librispeech), as well as community-trained models on French (Common Voice) and Spanish. SpeechBrain provides ready-to-use models for French and Spanish that were fine-tuned on those languages’ Common Voice data. One could load the French model for French input, the Spanish model for Spanish, etc. They are not a single model for all languages, but switching models is straightforward. In summary, wav2vec2 offers a **consistent architecture** with checkpoints available for each target language.
* **Decoding Method**: These models typically use **CTC (Connectionist Temporal Classification)** decoding. The acoustic model (the Transformer encoder) outputs frame-wise character or subword probabilities, and a CTC beam search or greedy decoder produces the transcript. By default (as in the SpeechBrain models), **no external LM is used** – the decoding is “lexicon-free,” relying purely on acoustic evidence. This means the transcriptions are more *literal* but also more prone to spelling or spacing quirks. One can optionally fuse an n-gram or Transformer LM to improve correctness, but that would start auto-correcting errors. For our purposes, using the no-LM version preserves mistakes: the model will output exactly the sequence of phonemes it believes it heard, mapped to letters. For instance, a Spanish learner saying *“estoy *contente**” (non-word) might get a transcript like “estoy contente” (since the model isn’t constrained to turn it into “contento”).
* **Streaming Capability**: A vanilla wav2vec2 model is not inherently streaming – it was trained on full sequences and the Transformer attention looks at the whole input. However, it can be **operated in near-real-time** by chunking the audio (e.g. processing 1-second frames in succession). There may be a small accuracy drop or need for overlap between chunks to avoid cutting in the middle of words. Projects like *torchaudio* and *ESPnet* have demonstrated chunk-wise streaming with wav2vec2. The model itself is fast enough: on CPU it can transcribe faster than real-time for short segments (especially the base model). For a truly live pipeline, one would implement a sliding window and possibly carry over the encoder’s context. In practice, these models can be used for real-time, just with a bit of engineering effort (and a slight latency of a few hundred milliseconds for buffering audio).
* **Accuracy vs. Literalness**: Wav2Vec2 fine-tuned models achieve good accuracy on their specific languages, though typically their WER is a bit higher than Whisper or the best models. For example, SpeechBrain’s French model has \~9.96% WER on CommonVoice test, and their Spanish model \~13.3% WER (Whisper large on the same might be \~5-6%). They handle normal speech well, but may falter with heavy accents or disfluencies. **Literalness** is a strong point: without a language model, they will output uncommon or incorrect words if that’s what the audio suggests. They do not automatically add punctuation or casing (output is usually all lowercase and unpunctuated, as is standard for ASR). If a speaker repeats a word or mispronounces something, the raw CTC output will reflect that (or possibly a sequence of characters that doesn’t form a valid word). In short, these models are reasonably accurate and tend to *not* correct grammar/spelling – which is useful for capturing learner errors, but you may see bizarre transcripts for unknown words.
* **Deployment**: The wav2vec2 models are available through **Hugging Face** and **SpeechBrain** repositories with permissive licenses. They run on PyTorch and can be exported to ONNX. Running on a PC (CPU or GPU) is straightforward – e.g., transcribing 1 second of audio takes a fraction of a second on a modern CPU. For mobile deployment, one could use Pytorch Mobile or convert the model to TFLite (some have done so for wav2vec2 base). The model is larger than RNNs, but quantization can help. Because there are fewer vendor-optimized runtimes for wav2vec2, one might use a general ONNX runtime on mobile. Nonetheless, its relatively high resource usage (hundreds of MB and heavy compute for large models) means that on-device use might favor the smaller versions or require a powerful mobile chipset. Toolkits like **SpeechBrain** provide easy inference interfaces – e.g., `EncoderASR.from_hparams(...)` can load a French or Spanish model in one line. Overall, these models are plug-and-play for offline use, with a trade-off in needing separate model files per language.

## Mozilla DeepSpeech / Coqui STT (RNN-CTC Model)

* **Model & Size**: *DeepSpeech* is a classic end-to-end ASR based on deep recurrent neural networks (RNNs). The architecture (DeepSpeech 1 & 2) uses multiple layers of Bidirectional LSTM/GRU to map audio to characters, trained with CTC loss. The English model released by Mozilla (\~DS 0.9) was about 47 million parameters and the model file \~180 MB. Coqui STT (the maintained fork) continues using similar architectures with some improvements. These models are relatively small and efficient compared to Transformers. They often come with an optional external language model. The acoustic model itself (without LM) can fit in <100 MB, and a 5-gram English LM might be another \~100 MB. Community-trained versions for other languages vary in size (generally tens of millions of params).
* **Language Support**: DeepSpeech was originally English-focused, but it **supports training on multiple languages**. Pretrained models for Spanish, French, etc., are available from the community (often trained on Common Voice data). For instance, Coqui provides models or checkpoints for Spanish and French. The toolkit is multilingual in that you can load different model files for each language. It does *not* have a single multilingual model – you use one model per language. This satisfies English, French, Spanish support, albeit via separate files. The ASR engine can be easily switched between models. (Mozilla’s project noted it is flexible and retrainable for various languages.)
* **Decoding Method**: Decoding is done with **CTC** as well, outputting character sequences. However, DeepSpeech traditionally relies on a **separate n-gram language model** to improve accuracy. During inference, the acoustic model’s CTC outputs are combined with an LM score to choose the most likely transcript. This means if the LM is enabled, the recognizer will favor valid words and grammatically likely sequences. For maximal literalness, one can disable the LM (or use a very weak LM). Running without the LM will output raw CTC results – which might include partial words or odd spellings, but will more directly reflect the audio. The trade-off is a higher error rate without the LM. In practice, many deployments use at least a small LM to ensure outputs are real words. It’s up to the developer to decide. (DeepSpeech’s default English model came with a 5-gram LM that could be toggled off.)
* **Streaming Capability**: One of the strengths of DeepSpeech/Coqui STT is real-time streaming recognition. The model processes audio frame by frame and can emit intermediate results. Since it’s an RNN, it inherently works left-to-right on streaming input. The codebase offers streaming APIs for microphone input. It’s been used on devices like Raspberry Pi for live transcription. One limitation noted historically was that the official Mozilla model was only tested on short (\~10 second) clips. However, Coqui has addressed some of these limitations to allow longer audio streaming. In any case, for typical sentence-length inputs or live dictation, it can handle streaming easily on a CPU.
* **Accuracy vs. Literalness**: These RNN models are older and generally **less accurate** than Transformer-based models. For example, on LibriSpeech test, DeepSpeech v0.9 WER was about \~7-10% (with LM). They may struggle more with heavy accents or noise. However, without an LM, they will output very literal phonetic transcripts. If a learner says an incorrect word, the acoustic model might output a phonetic approximation (which could be an OOV sequence). With the LM turned on, the system will almost always output a *real word* – effectively correcting the error to the nearest valid word. For instance, “I *goed* home” without LM might come out as “i goed home” (since “goed” isn’t in LM, it might even split it “go ed”). With the LM, it would likely correct to “i **went** home.” Thus, for pronunciation training, one would likely disable or minimize the LM. The model will then faithfully show weird outputs for mispronunciations – albeit at the cost of more noise and spelling issues in transcripts. In summary, the Coqui STT can preserve errors but only if you choose to not use the usual decoding enhancements.
* **Deployment**: This model is extremely deployable. It’s written in C++ (with Python bindings) and can run on CPU even on modest hardware. It has been successfully run on mobile – TensorFlow Lite versions of DeepSpeech models exist (the `.tflite` model can be as small as 50 MB for the acoustic model). Coqui provides **Android and iOS** examples for on-device use. The fact that models are smaller and use simpler operations (LSTMs, matrix mult) makes them feasible on ARM processors without GPU. Integration is straightforward: you can use Coqui’s Python API or the command-line to transcribe audio. There’s also a C++ library for embedding into applications. Many hobby projects choose Vosk or Coqui for offline recognition due to this ease. If you need to support all three languages, you would bundle three model files (one per language) and load the appropriate one – which is still quite practical (e.g. English 180MB + French \~200MB + Spanish \~200MB). In summary, DeepSpeech/Coqui STT is a lightweight, streaming-capable solution that trades some accuracy for simplicity and control.

## Vosk (Kaldi-Based Offline ASR)

* **Model & Size**: **Vosk** is an offline speech recognition toolkit built on Kaldi (traditional ASR). Under the hood it uses factorized *time-delay neural network* acoustic models (TDNNs) or similar, with WFST (weighted finite-state transducer) decoders. Vosk’s default models are highly compact: around \~50 MB per language for the “small” models. These small models are designed for embedded devices and have a limited vocabulary (for example, 50k words). Vosk also offers larger models (several hundred MB, with larger vocabulary and higher accuracy) for server or PC use. The small English model is \~50 MB, the large one \~1.8 GB; French model \~70 MB, Spanish \~60 MB (approximate figures). Thus, Vosk models are among the smallest, making them ideal for mobile/edge deployment.
* **Language Support**: Vosk supports **20+ languages and dialects**, including English, French, and Spanish. Each language has its own acoustic model and lexicon. You would choose the model corresponding to the input language. (Vosk does not auto-detect language, though you could run parallel recognizers to handle uncertain input.) Notably, Vosk covers many languages beyond our scope as well, from Chinese to Italian and beyond. For our use, the relevant models would be: *vosk-model-en-us*, *vosk-model-fr*, *vosk-model-es* (plus options like “small” or “large” variants).
* **Decoding Method**: The Vosk/Kaldi approach uses a **WFST decoder with an n-gram language model** built into the model graph. The acoustic model outputs phonetic likelihoods which are combined with the language model and a pronunciation lexicon to yield words. This means the output will almost always be a valid word from the vocabulary. If a speaker says something outside the vocabulary or jumbles a word, the decoder will guess the nearest match. This can sometimes *mask learner errors*. For example, if someone pronounces *“red”* as a non-word “rad”, a lexicon-based system might still output “red” because “rad” isn’t in the vocabulary (unless it is listed as slang). There is a possibility to modify the vocabulary or turn off the LM, but that’s not typical with Vosk. That said, Vosk’s decoding is relatively **literal within its vocabulary** – it doesn’t do advanced paraphrasing, it’s mostly constrained to choosing sequences of known words that fit the sounds. It won’t correct grammar beyond choosing a likely word sequence. For capturing mispronunciations, one could extend the lexicon with common mispronounced variants or phonetic spellings, but that requires some effort.
* **Streaming**: Vosk is **built for streaming**. It provides a streaming API in various languages (Python, Java, C#, etc.), allowing incremental decoding of live audio. It has low latency; partial results can be obtained as the user speaks. Because it’s efficient, it can run in real-time even on phones (there’s an Android demo). It processes audio in small chunks (e.g. 0.1s) and updates the decoding lattice. For mobile or embedded usage, Vosk’s streaming is a major advantage – it doesn’t require a GPU or heavy compute to keep up with human speech.
* **Accuracy vs. Literalness**: In terms of accuracy, Vosk’s large models are quite competitive (on par with Google’s 2017-era models, perhaps). The small models sacrifice some accuracy for size. For example, the large English model might get <10% WER on standard tests, while the tiny 50MB model might be higher. Compared to end-to-end models, Kaldi models can perform well in clean conditions but might be less robust to heavy noise or wildly different accents (since they have fixed vocabularies and are older-style acoustic models). Regarding *literalness*: Vosk will produce *verbatim output for what it thinks was said, constrained to real words*. Filled pauses like “um, uh” are typically modeled (they’re often in the lexicon), so those might appear. But if the speaker uses an incorrect word that *is* in the vocabulary, Vosk will output that incorrect word. If they utter something not in vocab, the system must output something else (possibly a word that was not intended). So it may not transcribe a mispronounced made-up word letter-by-letter; it will choose the closest known word. This behavior is slightly less ideal for our use, but the trade-off is that when it does output a word, you know it’s at least a valid word (even if it wasn’t the one the speaker meant). In summary, Vosk/Kaldi might *mask* some errors by forcing a valid output, but it won’t do high-level grammar fixes.
* **Deployment**: Vosk is extremely deployment-friendly. It’s just a **simple library (pip install vosk) with no external dependencies** needed for inference. The models are provided as archives that you load at runtime. Because models are small, they can be embedded in mobile app packages. There are official bindings for Java (Android), Swift (iOS), C# (Unity), and more. Many users have integrated Vosk for offline voice commands on phones and Raspberry Pi. Memory and CPU requirements are low – e.g., the 50MB English model uses about \~100 MB RAM and can run at >1x real-time on a single CPU core. This makes it ideal for always-on listening or on-device transcription without internet. The library supports multi-threading if needed for faster than real-time processing or running multiple languages in parallel. In conclusion, Vosk is a solid choice when you need lightweight, streaming ASR across English, French, Spanish, with the caveat that it uses a fixed language model (and thus might not preserve every learner slip-up). It is fully open-source (Apache-2.0 licensed) and widely used in embedded speech applications.

**Sources:**

1. OpenAI, *Introducing Whisper* – model architecture, multilingual training and tasks
2. Gladia blog – Whisper model sizes (39M–1.55B params) and accuracy notes
3. History of ASR (user file) – Whisper Large V3 details (1.55B params, 680k hrs training)
4. NVIDIA NeMo documentation – Canary and FastConformer model (1B & 114M params, 4 languages incl. EN/FR/ES)
5. NVIDIA NGC model card – Multilingual FastConformer RNNT (10 languages, \~20k hrs, 114M params)
6. SpeechBrain HuggingFace – French ASR (wav2vec2 + CTC, no LM, WER \~9.96%)
7. SpeechBrain HuggingFace – Spanish ASR (wav2vec2 + CTC, no LM, WER \~13.3%)
8. Mozilla DeepSpeech blog (Gladia) – DeepSpeech uses RNN + n-gram LM, multilingual and flexible but older tech
9. Vosk documentation – Supported languages (English, French, Spanish, etc.), model sizes (\~50 MB), and streaming API
===================================================================
